Create a sample dataset and implement the below Pig commands on the same dataset.
======================================
1) Concat 
======================================
A = LOAD 'data' as (f1:chararray, f2:chararray, f3:chararray);
 
DUMP A;
(apache,open,source)
(hadoop,map,reduce)
(pig,pig,latin)
 
X = FOREACH A GENERATE CONCAT(f2,f3);
 
DUMP X;
(opensource)
(mapreduce)
(piglatin)
 
==========================================
2) Tokenize 
========================================== 
A  = LOAD 'data' AS (f1:chararray);
 
DUMP A;
(Here is the first string.)
(Here is the second string.)
(Here is the third string.)
 
X = FOREACH A GENERATE TOKENIZE(f1);
 
DUMP X;
({(Here),(is),(the),(first),(string.)})
({(Here),(is),(the),(second),(string.)})
({(Here),(is),(the),(third),(string.)})
============================================
3)Sum 
============================================
A = LOAD 'data' AS (owner:chararray, pet_type:chararray, pet_num:int);
 
DUMP A;
(Alice,turtle,1)
(Alice,goldfish,5)
(Alice,cat,2)
(Bob,dog,2)
(Bob,cat,2)
 
B = GROUP A BY owner;
 
DUMP B;
(Alice,{(Alice,turtle,1),(Alice,goldfish,5),(Alice,cat,2)})
(Bob,{(Bob,dog,2),(Bob,cat,2)})
 
X = FOREACH B GENERATE group, SUM(A.pet_num);
DUMP X;
(Alice,8L)
(Bob,4L)
=================================================
4) Min 
=================================================
A = LOAD 'student' AS (name:chararray, session:chararray, gpa:float);
 
DUMP A;
(John,fl,3.9F)
(John,wt,3.7F)
(John,sp,4.0F)
(John,sm,3.8F)
(Mary,fl,3.8F)
(Mary,wt,3.9F)
(Mary,sp,4.0F)
(Mary,sm,4.0F)
 
B = GROUP A BY name;
 
DUMP B;
(John,{(John,fl,3.9F),(John,wt,3.7F),(John,sp,4.0F),(John,sm,3.8F)})
(Mary,{(Mary,fl,3.8F),(Mary,wt,3.9F),(Mary,sp,4.0F),(Mary,sm,4.0F)})
 
X = FOREACH B GENERATE group, MIN(A.gpa);
 
DUMP X
(John,3.7F)
(Mary,3.8F)
====================================================
5) Max
====================================================
A = LOAD 'student' AS (name:chararray, session:chararray, gpa:float);
 
DUMP A;
(John,fl,3.9F)
(John,wt,3.7F)
(John,sp,4.0F)
(John,sm,3.8F)
(Mary,fl,3.8F)
(Mary,wt,3.9F)
(Mary,sp,4.0F)
(Mary,sm,4.0F)
 
B = GROUP A BY name;
 
DUMP B;
(John,{(John,fl,3.9F),(John,wt,3.7F),(John,sp,4.0F),(John,sm,3.8F)})
(Mary,{(Mary,fl,3.8F),(Mary,wt,3.9F),(Mary,sp,4.0F),(Mary,sm,4.0F)})
 
X = FOREACH B GENERATE group, MAX(A.);
 
DUMP X;
(John,4.0F)
(Mary,4.0F)
=============================================================
 6) Limit 
===========================================================
A = LOAD 'data' AS (a1:int,a2:int,a3:int);
 
DUMP A;
(1,2,3)
(4,2,1)
(8,3,4)
(4,3,3)
(7,2,5)
(8,4,3)
 
X = LIMIT A 3;
 
DUMP X;
(1,2,3)
(4,3,3)
(7,2,5)
========================================================
7) Store
========================================================
A = LOAD 'data' AS (a1:int,a2:int,a3:int);
 
DUMP A;
(1,2,3)
(4,2,1)
(8,3,4)
(4,3,3)
(7,2,5)
(8,4,3)
 
STORE A INTO 'myoutput' USING PigStorage ('*');
 
CAT myoutput;
1*2*3
4*2*1
8*3*4
4*3*3
7*2*5
8*4*3
========================================================
8) Distinct 
=======================================================
A = LOAD 'data' AS (a1:int,a2:int,a3:int);
 
DUMP A;
(8,3,4)
(1,2,3)       
(4,3,3)        
(4,3,3)       
(1,2,3)
 
X = DISTINCT A;
 
DUMP X;
(1,2,3)
(4,3,3)
(8,3,4)
======================================================
9) Flatten 
======================================================

lines = LOAD '/user/hadoop/Test_File.txt' AS (line:chararray);
words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;
grouped = GROUP words BY word;
wordcount = FOREACH grouped GENERATE group, COUNT(words);
DUMP wordcount;
========================================================
10) IsEmpty
=========================================================
 
SSN = load 'ssn.txt' using PigStorage() as (ssn:long);
SSN_NAME = load 'students.txt' using PigStorage() as (ssn:long, name:chararray);
 
-- do a left out join of SSN with SSN_Name
X = cogroup SSN by ssn inner, SSN_NAME by ssn;
 
-- only keep those ssn's for which there is no name
Y = filter X by IsEmpty(SSN_NAME);
